{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings con OpenAI\n",
    "Embeddings es un proceso mediante el cual se utiliza alguna tecnica/algoritmo que sea capaz de convertir palabras o texto a vectores de N dimensiones. Estos vectores contienen cierto nivel de información semantica sobre el texto o palabra. Por ejemplo, palabras que son muy similares van a tener valores cercanos en sus representaciones en vectores.\n",
    "\n",
    "Hay varios modelos que son capaces de hacer un embedding de nuestro texto, en este cuaderno estaremos utilizando el embedding de OpenAI, el cual tiene la capacidad de posicionas muy bien palabras o textos segun su semantica. Este es el mismo embedding que utiliza GPT3. En este cuaderno estarás haciendo embedding de un texto para despues poder buscar cosas dentro de este texto por medio de preguntas en lenguaje natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importamos todas las dependencias requeridas, en este caso será Gradio para desarrollar la interfaz grafica y openai para realizar los llamados a su API \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "# Importamos todas las dependencias requeridas, en este caso será Gradio para desarrollar la interfaz grafica y openai para realizar los llamados a su API \n",
    "import gradio as gr\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from openai.embeddings_utils import get_embedding\n",
    "from openai.embeddings_utils import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la API Key para vincular el cuaderno con nuestra cuenta de OpenAI\n",
    "openai.api_key = \"sk-\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Que es y cómo usar embeddings\n",
    "Al hacer embedding de un dato, lo estamos convirtiendo a un vector numérico, datos similares estarán más cercanos entre si cuando semanticamente son similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se puede hacer embeeding de palabras o cadenas de texto\n",
    "palabras = [\"casa\", \"perro\", \"gato\", \"lobo\", \"leon\", \"zebra\", \"tigre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario = {}\n",
    "for i in palabras:\n",
    "    diccionario[i] = get_embedding(i, engine=\"text-embedding-ada-002\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['casa', 'perro', 'gato', 'lobo', 'leon', 'zebra', 'tigre'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diccionario.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 10 valores de gato:\n",
      " [-0.024576973170042038, 0.009653845801949501, -0.0003061550669372082, -0.011156566441059113, -0.007181836757808924, 0.01183311641216278, -0.022013889625668526, -0.024954279884696007, -0.0052172401919960976, -0.019047480076551437]\n",
      "\n",
      "\n",
      "Número de dimensiones del dato embebido\n",
      " 1536\n"
     ]
    }
   ],
   "source": [
    "palabra = \"gato\"\n",
    "print(\"Primeros 10 valores de {}:\\n\".format(palabra), diccionario[palabra][:10])\n",
    "print(\"\\n\")\n",
    "print(\"Número de dimensiones del dato embebido\\n\", len(diccionario[palabra]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar dos embeddings\n",
    "Debido a que los embeddings son una representacion vectorial de los datos en un espacio latente, podemos medir la distancia entre dos vectores y asi obtener que tan similares son. Podemos comparar una palabra nueva o alguna de las que ya fueron embebidas \n",
    "OJO: No necesariamente es similitud al objeto. Ej. perro y gato aun siendo \"opuestos\" semanticamente estan cerca pues tienen una relación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8159836735319491\n"
     ]
    }
   ],
   "source": [
    "n_palabra = \"agujero negro\" # Palabra nueva a comparar\n",
    "palabra_comparar = \"perro\" # Palabra del diccionario con la que compararemos la nueva palabra\n",
    "n_palabra_embed = get_embedding(n_palabra, engine=\"text-embedding-ada-002\")\n",
    "similitud = cosine_similarity(diccionario[palabra_comparar], n_palabra_embed)\n",
    "print(similitud)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumar embeddings\n",
    "Como los vectores contienen valores numericos, podemos sumarlos y el resultado será un nuevo vector de un concepto que una los elementos sumados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casa : [0.82819415]\n",
      "perro : [0.85046401]\n",
      "gato : [0.85630059]\n",
      "lobo : [0.85639036]\n",
      "leon : [0.9531031]\n",
      "zebra : [0.95310309]\n",
      "tigre : [0.88060081]\n"
     ]
    }
   ],
   "source": [
    "# Suma dos listas usando pandas\n",
    "sumados = (pd.DataFrame(diccionario[\"leon\"])) + (pd.DataFrame(diccionario[\"zebra\"]))\n",
    "len(sumados)\n",
    "\n",
    "for key, value in diccionario.items():\n",
    "    print(key, \":\", cosine_similarity(diccionario[key], sumados))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicacion de un Chatbot\n",
    "\n",
    "Usaremos Gradio para hacer una interfaz básica donde podremos hacer preguntas y obtendremos una respuesta. \n",
    "Para esto reutilizaremos lo que hemos visto hasta el momento pero usaremos el archivo de **chatbot_qa.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embed_text(path=\"texto.csv\"):\n",
    "    conocimiento_df = pd.read_csv(path)\n",
    "    conocimiento_df['Embedding'] = conocimiento_df['texto'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
    "    conocimiento_df.to_csv('embeddings.csv')\n",
    "    return conocimiento_df\n",
    "\n",
    "def buscar(busqueda, datos, n_resultados=5):\n",
    "    busqueda_embed = get_embedding(busqueda, engine=\"text-embedding-ada-002\")\n",
    "    datos[\"Similitud\"] = datos['Embedding'].apply(lambda x: cosine_similarity(x, busqueda_embed))\n",
    "    datos = datos.sort_values(\"Similitud\", ascending=False)\n",
    "    return datos.iloc[:n_resultados][[\"texto\", \"Similitud\", \"Embedding\"]]\n",
    "\n",
    "texto_emb = embed_text(\"./chatbot_qa.csv\")\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    busqueda = gr.Textbox(label=\"Buscar\")\n",
    "    output = gr.DataFrame(headers=['texto'])\n",
    "    greet_btn = gr.Button(\"Preguntar\")\n",
    "    greet_btn.click(fn=buscar, inputs=[busqueda, gr.DataFrame(texto_emb)], outputs=output)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesar datos de un PDF\n",
    "Haremos ahora un ejemplo donde leemos un PDF para poder hacer preguntas y traer un exctracto del PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"doc/I-CP-01 (A).pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SIN CLASIFICAR \\n1-CP-01 (A) \\nEl Almirante de la Flota \\nFebrero 2015 \\nORDEN DE PROMULGACION \\nl. Conforme a lo dispuesto en la Instrucción Permanente de Organización núm. \\n10/2010 sobre la DOCTRINA EN LA ARMADA (CAMBIO 3), se declara \\nreglamentaria la publicación I-CP-0 1 (A) \"TÉCNICAS Y MATERIALES DE \\nCONTRAINCENDIOS\". \\n2. La I-CP-Ol(A) es una publicación SIN CLASIFICAR y entrará en vigor a su \\nrecepción. \\n3. La I-CP-Ol(A) anula y sustituye a la publicación I-CP-01 (CAMBIO 1), que se \\ndestruirá de acuerdo a las disposiciones en vigor. \\n--------\\n111 ORIGINAL \\nSIN CLASU=ICAR'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Un elemento por cada página\n",
    "pages[3].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objeto que va a hacer los cortes en el texto\n",
    "split = CharacterTextSplitter(chunk_size=300, separator = '.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1312, which is longer than the specified 300\n",
      "Created a chunk of size 1803, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "textos = split.split_documents(pages) # Lista de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIN CLAS IFICAR  \n",
      " \n",
      "SIN CLAS IFICAR  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "I-CP-01 (A) \n",
      " \n",
      "  \n",
      "TECNICAS Y  \n",
      "MATERIALES DE \n",
      "CONTRAINCENDIOS\n"
     ]
    }
   ],
   "source": [
    "print(textos[0].page_content)\n",
    "#print(textos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 texto\n",
      "0    SIN CLAS IFICAR  \\n \\nSIN CLAS IFICAR  \\n  \\n ...\n",
      "1    SIN CLAS IFICAR  \\nI-CP-01 (A) \\n   \\n \\n  \\nO...\n",
      "2    SIN CLAS IFICAR  \\nI-CP-01 (A) \\n   \\n \\n  \\nO...\n",
      "3    SIN CLASIFICAR \\n1-CP-01 (A) \\nEl Almirante de...\n",
      "4    SIN CLASIFICAR \\n1-CP-01 (A) \\nPÁGINA EN BLANC...\n",
      "..                                                 ...\n",
      "318  SIN CLAS IFICAR  \\nI-CP-01(A) \\n   \\n  \\nORIGI...\n",
      "319  SIN CLAS IFICAR  \\nI-CP-01(A) \\n   \\n  \\nORIGI...\n",
      "320  SIN CLAS IFICAR  \\nI-CP-01(A)  \\n \\nORIGINAL  ...\n",
      "321  SIN CLAS IFICAR  \\nI-CP-01(A)  \\n \\nORIGINAL  ...\n",
      "322  SIN CLAS IFICAR  \\nSIN CLASIFICAR   \\n        ...\n",
      "\n",
      "[323 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extraemos la parte de page_content de cada texto y lo pasamos a un dataframe\n",
    "textos = [str(i.page_content) for i in textos] #Lista de parrafos\n",
    "parrafos = pd.DataFrame(textos, columns=[\"texto\"])\n",
    "print(parrafos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Cargar el modelo Universal Sentence Encoder (versión 4)\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Obtener embeddings para los párrafos\n",
    "try:\n",
    "    embeddings = embed(parrafos[\"texto\"])\n",
    "except Exception as e:\n",
    "    print(f\"Error al obtener embeddings: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Agregar los embeddings al DataFrame\n",
    "parrafos['Embedding'] = tf.convert_to_tensor(embeddings).numpy().tolist()\n",
    "\n",
    "# Guardar el DataFrame con los embeddings en un archivo CSV (verifica si existe)\n",
    "nombre_archivo = \"emb-SI.csv\"\n",
    "if os.path.exists(nombre_archivo):\n",
    "    nombre_archivo = f\"{nombre_archivo}-{int(time.time())}\"\n",
    "\n",
    "parrafos.to_csv(nombre_archivo, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La misma funcion del chatbot de pregunts y respuestas\n",
    "def embed_text(path=\"texto.csv\"):\n",
    "    conocimiento_df = pd.read_csv(path)\n",
    "    conocimiento_df['Embedding'] = conocimiento_df['texto'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
    "    conocimiento_df.to_csv('mtg-embeddings.csv')\n",
    "    return conocimiento_df\n",
    "\n",
    "def buscar(busqueda, datos, n_resultados=5):\n",
    "    busqueda_embed = get_embedding(busqueda, engine=\"text-embedding-ada-002\")\n",
    "    datos[\"Similitud\"] = datos['Embedding'].apply(lambda x: cosine_similarity(x, busqueda_embed))\n",
    "    datos = datos.sort_values(\"Similitud\", ascending=False)\n",
    "    return datos.iloc[:n_resultados][[\"texto\", \"Similitud\", \"Embedding\"]]\n",
    "\n",
    "texto_emb = parrafos\n",
    "with gr.Blocks() as demo:\n",
    "    busqueda = gr.Textbox(label=\"Buscar\")\n",
    "    output = gr.DataFrame(headers=['texto'])\n",
    "    greet_btn = gr.Button(\"Preguntar\")\n",
    "    greet_btn.click(fn=buscar, inputs=[busqueda, gr.DataFrame(texto_emb)], outputs=output)\n",
    "\n",
    "demo.launch()\n",
    "\n",
    "\n",
    "# resp = buscar(\"Con cuanta vida empiezo?\", parrafos, 5) # Reutilizamos la funcion de \"buscar\" del app de gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp = buscar(\"Con cuanta vida empiezo?\", parrafos, 5) # Reutilizamos la funcion de \"buscar\" del app de gradio\n",
    "# print(resp.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('gpt3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3e99415c7ce2cb9b8857283077fa90c1d7ffec737ddc9f365b1225d7f13a404"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
